<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wadmes.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"buttons","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta name="description" content="前言。本文作于GCN横空出世（2017）的三年后（2020），想尽可能在简单易懂的情况下说清楚近年GNN的发展脉络。当然，凭我鸽子王的尿性，估计本来也就不会写的很翔实（简单是第一层，偷懒是第二层🐶）。  最近GNN又看了一些ICLR2020的paper，加之之前积累的paper，对整个GNN的发展脉络有了比较清楚的理解，之所以想要记录在案，是因为发现：  GNN有点类似CNN，是一个入门难度不">
<meta property="og:type" content="article">
<meta property="og:title" content="超精简的GNN综述">
<meta property="og:url" content="http://wadmes.github.io/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="NEWAY">
<meta property="og:description" content="前言。本文作于GCN横空出世（2017）的三年后（2020），想尽可能在简单易懂的情况下说清楚近年GNN的发展脉络。当然，凭我鸽子王的尿性，估计本来也就不会写的很翔实（简单是第一层，偷懒是第二层🐶）。  最近GNN又看了一些ICLR2020的paper，加之之前积累的paper，对整个GNN的发展脉络有了比较清楚的理解，之所以想要记录在案，是因为发现：  GNN有点类似CNN，是一个入门难度不">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=H%5E%7B(l%252B1)%7D+%253D+%5Csigma+(%5Cwidetilde+D+%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+%5Cwidetilde+A+%5Cwidetilde+D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+H%5E%7B(l)%7DW%5E%7B(l)%7D+)+%5C%5C+%5C%5C">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=a_%7Blearn%7D(i%252Cj)%253D%5Cfrac%7Bexp(LeakyReLU(w%5BWx_i%252CWx_j%5D))%7D%7B%5CSigma_%7Bk%5Cin+neigh(i)%7Dexp(LeakyReLU(w%5BWx_i%252CWx_k%5D))%7D%5C%5C">
<meta property="og:image" content="http://wadmes.github.io/figures/image-20200627214400158.png">
<meta property="og:image" content="http://wadmes.github.io/figures/v2-7a1237527813b338926f6632bea84720_b.jpg">
<meta property="og:image" content="http://wadmes.github.io/figures/image-20200819215408321.png">
<meta property="og:image" content="http://wadmes.github.io/figures/image-20200819215427294.png">
<meta property="og:image" content="http://wadmes.github.io/figures/image-20200819215438565.png">
<meta property="article:published_time" content="2020-06-26T17:01:52.000Z">
<meta property="article:modified_time" content="2020-09-23T12:46:12.570Z">
<meta property="article:author" content="Li Wei">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="GNN">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=H%5E%7B(l%252B1)%7D+%253D+%5Csigma+(%5Cwidetilde+D+%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+%5Cwidetilde+A+%5Cwidetilde+D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+H%5E%7B(l)%7DW%5E%7B(l)%7D+)+%5C%5C+%5C%5C">

<link rel="canonical" href="http://wadmes.github.io/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>超精简的GNN综述 | NEWAY</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">NEWAY</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">愿识乾坤大，仍怜草木青</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-reading-notes">

    <a href="/notes" rel="section"><i class="fas fa-book-reader fa-fw"></i>Reading notes</a>

  </li>
        <li class="menu-item menu-item-about-me">

    <a href="/about" rel="section"><i class="fas fa-user fa-fw"></i>About me</a>

  </li>
        <li class="menu-item menu-item-official-about-me">

    <a href="/cv" rel="section"><i class="fas fa-user-secret fa-fw"></i>Official About me</a>

  </li>
        <li class="menu-item menu-item-resume">

    <a href="/cv/cv_liwei/cv.pdf" rel="section"><i class="fas fa-file fa-fw"></i>Resume</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://wadmes.github.io/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/liwei3.jpg">
      <meta itemprop="name" content="Li Wei">
      <meta itemprop="description" content="人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NEWAY">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          超精简的GNN综述
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-27 01:01:52" itemprop="dateCreated datePublished" datetime="2020-06-27T01:01:52+08:00">2020-06-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-23 20:46:12" itemprop="dateModified" datetime="2020-09-23T20:46:12+08:00">2020-09-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/study/" itemprop="url" rel="index"><span itemprop="name">study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/06/27/%E8%B6%85%E7%B2%BE%E7%AE%80%E7%9A%84GCN%E7%BB%BC%E8%BF%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>前言。本文作于GCN横空出世（2017）的三年后（2020），想尽可能在简单易懂的情况下说清楚近年GNN的发展脉络。当然，凭我鸽子王的尿性，估计本来也就不会写的很翔实（简单是第一层，偷懒是第二层🐶）。</p>
</blockquote>
<p>最近GNN又看了一些ICLR2020的paper，加之之前积累的paper，对整个GNN的发展脉络有了比较清楚的理解，之所以想要记录在案，是因为发现：</p>
<ul>
<li>GNN有点类似CNN，是一个入门难度不高，效果又比较出色的技术。不像EDA里面各种子问题及现在的SOTA解决方案，要解释清楚就要半天。</li>
<li>现在的各种文章因为作者们的严谨及数学功底的雄厚，导致涉及内容过多，我要真正看明白的阈值太高（对不起实在是因为我太菜了），因此心里琢磨着要是能有一篇浅显易懂的综述，快捷的理清paper间的区别和发展脉络该多好。</li>
<li>现在许多GNN的paper其实真正technique部分的contribution很容易概括，当然所谓的research也不可能是一蹴而就 novelty、results什么的全都完爆previous work。只是很多paper的出发点都不一样，导致看起来paper间差异很大，其实在我看来现在GNN还是一个比较混沌的状态，等着哪位X kaiming大神来盘古开天。（或者说GCN这篇就算是开天了？）</li>
</ul>
<p><strong>Note：这篇文章完全是为我本人量身打造，是为了未来的我复习GNN的时候有个参考。因此我面向的读者是有了比较充足的CNN、Graph知识背景的。</strong></p>
<h1 id="开山老祖-GCN"><a href="#开山老祖-GCN" class="headerlink" title="开山老祖 GCN"></a>开山老祖 GCN</h1><blockquote>
<p>这一段大部分是我的臆测。。可以直接看结论。</p>
</blockquote>
<p>给一个irregular的grpah/point cloud，我们可以从频域(specture)和空间(spatial)角度去解决这个问题</p>
<ul>
<li>频域角度就是图信号映射到频域后再做卷积操作。（直接copy from zhihu，原谅我这一块其实并不是很清楚，因为现在的大部分work都不会提specture这个东西了。。。而且本身这个对数学功底要求就有点高，也许我真的phd了可以花时间研究这个？单就目前纯水paper的角度，这个建议不要碰，1.上手难，2.现在大部分SOTA都是spatial）</li>
<li>空间角度大概就是直接从点/线的关系入手，用这些关系来表示某种message passing。messgae passing的意思很简单，就是说给一个点，它会把它自己的信息（feature）传输给它的邻点，同样，它的邻点也会把邻点的信息传给它。大概有点像社会学里面的信息传输，譬如李巍长得很帅，一传十十传百所有人都知道我长得很帅这样。。（我长得很帅这个信息就被所有人收到了，这样在他们那的信息就是：我有个朋友/朋友的朋友长的很帅）</li>
</ul>
<p>这篇GCN做的就是，从频域角度出发给了GCN的公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=H%5E%7B%28l%2B1%29%7D+%3D+%5Csigma+%28%5Cwidetilde+D+%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+%5Cwidetilde+A+%5Cwidetilde+D%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D+H%5E%7B%28l%29%7DW%5E%7B%28l%29%7D+%29+%5C%5C+%5C%5C" alt="[公式]"></p>
<p>$H^l$就是第l层的所有点的feature，$\bar{A}$是加上identity matrix的矩阵，D是degree matrix为了normalization（结果其实比较接近mean），W就是映射到另一个空间增加信息量。</p>
<p>然后发现这个公式其实可以从空间的角度很elegant的解释，GCN的每一层其实就是aggragation + combing（encoding）</p>
<ul>
<li>$\bar{A}H$ 其实就是对于任何一个点，将自己的feature，以及邻点的feature 累加起来，加个D就是为了normalization（$D^{-1}$ 其实就是mean），因为每次都累加层数多了这个值不爆炸了？而且有些点邻居本来就多怎么办？。</li>
<li>$W^{l}$ 就是做一次combing啦，可以理解成投射到另一个空间增加复杂度（其实就是1*1conv，or FC，or MLP，哦，有的可能还会加个bias）</li>
</ul>
<h1 id="方法层面"><a href="#方法层面" class="headerlink" title="方法层面"></a>方法层面</h1><h2 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h2><p>这篇就直接不管specture了，直接从空间角度把GCN重新提了一遍，同时formally定义了aggregator这个概念，其实就是对于每个点，怎么将它自己及它邻点的信息聚合（aggregate）起来。然后还提出了几个方法。</p>
<ul>
<li>不加自己的feature mean 。</li>
<li>加上自己的feature mean。。。</li>
<li>LSTM。。。也许那个年代LSTM比较火吧。</li>
<li>channel-wise的max sampling</li>
</ul>
<p>注：aggregator其实总之不管是什么，都只要保证order-invarienty以及size-insensitivity就行了。。</p>
<h2 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h2><p>这篇就是说attention效果这么好，我们也套用到GCN吧！（对不起这是我的小人之心🐶），实际上应该是：每个邻居可能重要性不一样，所以要加个attention！</p>
<p><img src="https://www.zhihu.com/equation?tex=a_%7Blearn%7D%28i%2Cj%29%3D%5Cfrac%7Bexp%28LeakyReLU%28w%5BWx_i%2CWx_j%5D%29%29%7D%7B%5CSigma_%7Bk%5Cin+neigh%28i%29%7Dexp%28LeakyReLU%28w%5BWx_i%2CWx_k%5D%29%29%7D%5C%5C" alt="[公式]"></p>
<p>a(i,j)就是点i对点j的重要性。但这个真把我秀到了，具体可以参见<a href="https://wadmes.github.io/2019/12/30/attention/">https://wadmes.github.io/2019/12/30/attention/</a></p>
<h2 id="JKNet"><a href="#JKNet" class="headerlink" title="JKNet"></a>JKNet</h2><p>这篇就是说DenseNet效果这么好，我们也套用到GCN吧！（其实background还是GCN层数过深，例如depth&gt;2，就会出现oversmooth的情况，即所有离得近点的feature长的都挺像的。而离得远的点互相的connection又太小了，所以加个densenet就可以让他们connection不小了呀呀呀呀呀呀）</p>
<blockquote>
<p>提到oversmoothing了就说下，其实直接原因是目前GCN的aggregation必须保证order invarienty，导致不像CNN里面的convolution kernel一样卷积时候可以带weight，想象一下CNN里面a pixel的右边是b pixel，传信息的时候用的是3*3kernel里面的(2,3)value，而b要传给a，就是(2,1)了，两个值不一样，所以不管传多少次都不会最后比较接近，除非(2,3) = (1,3)，这就是GCN现在的情况，因为GCN不可能还有方向这个概念，或者说，给一个点a，他有邻居b，c，目前还不可能能分出b，c。。</p>
</blockquote>
<h2 id="DeepGCN"><a href="#DeepGCN" class="headerlink" title="DeepGCN"></a>DeepGCN</h2><p>这篇就是说ResNet效果这么好，我们也套用到GCN吧！</p>
<p>然后除了简单的加个residual connection，还提出了一个GCN上的dialated convolution，个人觉得是比residual 更重要/有效的一点。大概就是：regular CNN里面dialated convolution可以很轻松的跳一个选一个这样卷积，但是GCN不好操作因为每个点邻居个数都不一样，而且还是无序的。这里的做法就是对于每个当前点，给其他点按照feature的欧几里得距离来个排序，然后就可以跳一个选一个啦！（其实有点做point cloud里面dynamic graph CNN那味了）</p>
<h2 id="Dropedge"><a href="#Dropedge" class="headerlink" title="Dropedge"></a>Dropedge</h2><p>这篇就是说Dropout效果这么好，我们也套用到GCN吧！</p>
<p>不过这里的dropout其实不同于CNN里面对feature的dropout（其实对feature的dropout在original GCN就用了），而是对weight的dropout，即每一层有概率的remove edge，（脑补一下，其实就相当于把GCN的weight全为1 改成了有些为0有些为1啊！）</p>
<h2 id="Position-aware-Graph-Neural-Networks"><a href="#Position-aware-Graph-Neural-Networks" class="headerlink" title="Position-aware Graph Neural Networks"></a>Position-aware Graph Neural Networks</h2><p>这个不是借用CNN的一些technique的理念，说下大概思路吧。</p>
<ul>
<li><p>问题：GCN只能捕捉局部的structure 信息，比如：</p>
<p><img src="/figures/image-20200627214400158.png" alt="image-20200627214400158"></p>
<p>不管怎样$v_1,v_2$（structurally isomorphic nodes）都是分配同样的label。那么怎么解决这个问题呢？</p>
<blockquote>
<p> node position can be captured by a low-distortion embedding by quantifying the distance between a given node and a set of anchor nodes</p>
</blockquote>
</li>
<li><p>先说下workflow吧，</p>
<ul>
<li>给$clog^2n$个子集$S_{i,j}，i=1…\log n,j=1,…,c\log n$，每个子集会随机选择所有点，每个点被选中的概率为$1/2^i$,所以其实子集的大小其实是指数型增长的</li>
<li>PGNN的每一层operation如下，<ul>
<li>对于每个点，和每个子集里面的所有点（假设n个点）的featrue(假设d维) concat 起来-&gt;$n\times 2d$，如果子集里的点是距离为$k$外的（太远了），则return  zero值（而不是concat feature）。接着把concat结果aggreate起来-&gt; $2d$ （注意这里的aggregate也需要满足order invarianty！所以也只能用mean，max，sum等，作者直接用的mean）</li>
<li>对于不同的子集的结果再用aggregate $c\log^2n \times 2d \rightarrow 2d$。</li>
</ul>
</li>
</ul>
</li>
<li><p>总结：这篇的theorem用了Bourgain Theorem去证明只要$c\log ^2 n$个就子集就可以preserve the distances in the original graph with low distortion。。。不过很多paper里面的theorem都是比较巧妙的，让人觉着遥不可及。但是这个work解决GCN只有local structrue的大概方法也是类似的：就是想方设法的介绍一些global information进去，这里是通过介绍子集的方法，相当于给子集中的每个点多了一个额外信息：属于哪个子集。打个比方，假设所有人只有性别这一个label，我和坤坤都只有三个男性朋友，一个女性朋友，这样一层GCN就无法通过信息传输判断我的label了，但是如果创造2个子集（身份），分别是 爱rap的同学 和 爱打篮球的同学，然后发现我有一个爱rap的同学，2个爱打篮球的同学，坤坤有3个爱rap的同学，这样我和坤坤的label就是不一样的了。</p>
</li>
</ul>
<h2 id="COLORING-GRAPH-NEURAL-NETWORKS-FOR-NODE-DISAMBIGUATION"><a href="#COLORING-GRAPH-NEURAL-NETWORKS-FOR-NODE-DISAMBIGUATION" class="headerlink" title="COLORING GRAPH NEURAL NETWORKS FOR NODE DISAMBIGUATION"></a>COLORING GRAPH NEURAL NETWORKS FOR NODE DISAMBIGUATION</h2><p>引入了universal representations &amp; separability的概念，claim MPNN不是一个universal representation。解决方法（甚至出发点）和上篇paper有点相似。大概就是给node attribute 相同的点 从k种颜色中随机选一个颜色标上，k为hyper parameter，这样就实现了universal approximation。有两个concern</p>
<ul>
<li>若k不为 graph size （即所有点颜色都不一样），那么这种coloring sheme不是permutation invariance。这也是ICLR被拒的原因。</li>
<li>k=1时，不就是所有点assign一个相同的颜色么。。没看懂。。。</li>
</ul>
<h2 id="GeomGCN"><a href="#GeomGCN" class="headerlink" title="GeomGCN"></a>GeomGCN</h2><p>同样，这个也不是借用CNN，说下大概思路吧。</p>
<ul>
<li>问题：1. 就像GIN说的（就这篇下面👇），现在基于聚合的都是垃圾，因为会将部分Non-isomorphic的图也给出同样的结果，如</li>
</ul>
<p>  <img src="/figures/v2-7a1237527813b338926f6632bea84720_b.jpg" alt="img"></p>
<ol>
<li>同样的 有些graph里面可能有些子图其实是有相似结构的，那么为什么不利用起来呢？</li>
</ol>
<ul>
<li><p>基于以上两点，可以大概构思怎么解决 1. 将不同的node 当作不同的neighbor来聚合 2. 给每个node 加上一个能表示局部结构的node （这样局部结构相同的两点其实就可以share同样的feature了）</p>
</li>
<li><p>所以大概workflow如下：</p>
<ul>
<li>给每个点计算latent space上的node embedding，注意此处的node embedding是仅仅包含local struture 信息的（原文用了一些GCN之前的获取node embedding的方法，真的很纳闷为什么不直接用SOTA的GCN来获取node embedding）</li>
<li>对于每个点centroid，它都有两种邻居：真实邻居，转化成latent space下的邻居（另一个时空下的恋爱🐶），然后将每个邻居按照与centroid在latent space中的位置关系分为不同的邻居（为了解决第一个问题）。这里的实验latent space是二维的，所以邻居只分了四种，左上右上左下右下。</li>
<li>这样对于每种邻居，我们就分了四种点，这样就有四个aggregator，最后四个aggregator结果concat起来就行（因为总得保证是固定个结果嘛）</li>
</ul>
</li>
</ul>
<h2 id="Generalization-and-Representational-Limits-of-Graph-Neural-Networks"><a href="#Generalization-and-Representational-Limits-of-Graph-Neural-Networks" class="headerlink" title="Generalization and Representational Limits of Graph Neural Networks"></a>Generalization and Representational Limits of Graph Neural Networks</h2><p>定义decide：一个GNN Q如果decide了一个图性质P，则说明图性质P不同的两个图，由Q产生出来的graph embedding也肯定不同。</p>
<p>这篇文章前半部分就是用一些例子证明了：</p>
<ul>
<li>（用一些反例）基于聚合的很多GNN不能decide很多很简单的性质。</li>
</ul>
<h1 id="理论层面"><a href="#理论层面" class="headerlink" title="理论层面"></a>理论层面</h1><h2 id="GIN"><a href="#GIN" class="headerlink" title="GIN"></a>GIN</h2><p>这个算是个人觉得最有意思的一篇了，大概就是说在座各位基于这种聚合的都是垃圾，最多也只能和从前的WL Test一样。<a href="https://wadmes.github.io/2019/09/14/GNN-GIN-GMN-study/">https://wadmes.github.io/2019/09/14/GNN-GIN-GMN-study/</a></p>
<h2 id="THE-LOGICAL-EXPRESSIVENESS-OF-GRAPH-NEURAL-NETWORKS"><a href="#THE-LOGICAL-EXPRESSIVENESS-OF-GRAPH-NEURAL-NETWORKS" class="headerlink" title="THE LOGICAL EXPRESSIVENESS OF GRAPH NEURAL NETWORKS"></a>THE LOGICAL EXPRESSIVENESS OF GRAPH NEURAL NETWORKS</h2><p>这篇（ICLR2020）大概就是接着上一篇GIN（ICLR2019）说，诶，确实以前的聚合（自己的+邻居的）都是垃圾，但我这个不一样了，对于每个点还有了个全局（readout）信息。</p>
<p>首先先从一个最简单的给点二分类的问题说起：如果要判断一个点是不是isolated node。我们是不是就没法正确分类了？因为true的点肯定没法传输信息。这个例子实在是妙啊。。</p>
<p>那么这个全局信息是什么呢？就是所有node feature的sum啦。。（ps。。头皮发麻）</p>
<h2 id="WHAT-GRAPH-NEURAL-NETWORKS-CANNOT-LEARN-DEPTH-VS-WIDTH"><a href="#WHAT-GRAPH-NEURAL-NETWORKS-CANNOT-LEARN-DEPTH-VS-WIDTH" class="headerlink" title="WHAT GRAPH NEURAL NETWORKS CANNOT LEARN: DEPTH VS WIDTH"></a>WHAT GRAPH NEURAL NETWORKS CANNOT LEARN: DEPTH VS WIDTH</h2><p>这篇文章的作者。。emm。。怎么说呢，一看就不是国人同时又是为计算机基础知识特别好的大神。</p>
<p>文章的证明需要比较强的计算机背景，不过结论很简洁易懂：</p>
<blockquote>
<p>该研究证明，如果我们想让 GNN 为常见的图问题（如环检测、直径估计、顶点覆盖等）提供解决方案，则节点嵌入的维度（即网络宽度 w）与层数（即网络深度 d）的乘积应与图大小 n 成正比，即 dw = O(n)。</p>
</blockquote>
<h2 id="Approximation-Ratios-of-Graph-Neural-Networks-for-Combinatorial-Problems"><a href="#Approximation-Ratios-of-Graph-Neural-Networks-for-Combinatorial-Problems" class="headerlink" title="Approximation Ratios of Graph Neural Networks for Combinatorial Problems"></a>Approximation Ratios of Graph Neural Networks for Combinatorial Problems</h2><p>这篇将GNN和distributed local algorithms 结合了起来，先证明了GNN和distributed local algorithms 是等价的，然后利用以前work对distributed local algorithms的研究得出的结论直接给出了GNN的一些结论。</p>
<p>大概就是现在两种GNN， MB-GNN 和 SB-GNN都比不上 VVC-GNN，定义分别是：</p>
<p><img src="/figures/image-20200819215408321.png" alt="image-20200819215408321"></p>
<p><img src="/figures/image-20200819215427294.png" alt="image-20200819215427294"></p>
<p><img src="/figures/image-20200819215438565.png" alt=""></p>
<p>因此，提出了一个VVC-GNN。大概意思就是给别一个边的两个出口都assign一个port，使得所有点给它邻点的信息完全不一样。</p>
<p>最后实验就是用这个model + reinforcement learning 证明了 这个model可以解决很简单的组合优化问题，但是MB，SB都不可以。</p>
<p>在我看来，有个点是没解决的：</p>
<ul>
<li>这种方法注定导致不能迁移到其他graph（相当于给每个edge的port一个人为的index）</li>
<li>这个方法训练时间需要很久，但同时不能用到其他graph。。也太扯了吧。</li>
</ul>
<p>关于distributed local algorithms的定义，请参考 <a href="https://arxiv.org/pdf/1205.2051.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1205.2051.pdf</a></p>
<h2 id="Distance-Encoding-–-Design-Provably-More-Powerful-Graph-Neural-Networks-for-Structural-Representation-Learning"><a href="#Distance-Encoding-–-Design-Provably-More-Powerful-Graph-Neural-Networks-for-Structural-Representation-Learning" class="headerlink" title="Distance Encoding – Design Provably More Powerful Graph Neural Networks for Structural Representation Learning"></a>Distance Encoding – Design Provably More Powerful Graph Neural Networks for Structural Representation Learning</h2><p>添加了一个distance encoding to encode certain distance from S to a node u， 其中S是图中的一部分structure，想学的target structure feature就是S的feature，若S为全图，则为全图的feature(graph embedding). </p>
<p>这里的distance encoding 有比较复杂的formal形式，但是实验里面用的其实就是最简单的两点之间最短距离。</p>
<h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>写完后，忽然发现了一个也可以算巧合的规律吧？所有方法层面的work基本都是国人lead，但是理论层面的work却寥寥无几。这算不算是一种悲哀呢？方法层面的东西固然重要、效果好，容易发paper，但是理论层面的才是有可能真正推动时代进步的那个钥匙，不过似乎大家都对这种“不好发paper”、“没有确定性”、“难入门”的东西有一种共识性的排斥，算是一种悲哀吧！</p>
<p>其实自己何尝不是这样呢。做了五六个work，除了openmpl，其他的基本都是比较讨巧，迎着市场需求（学术圈热点）做的东西。这是这个时代的无奈，也是我个人的无奈吧，其实也是蛮想做一个沉下心学习研究一两年然后真正contribute to the development of CS的东西，可是现在还只是一个再过几个月要靠publication去砸PHD commitee脸的小小mphi；就算phd了也只是一个担心qualification、担心graduation的PHD student/candidate；就算当教授了也需要paper去争tenure啊；就算当了professor，也得对学生负责，在他没有成果前不可能让他做一些我自己都无法保证的东西吧？好像真正可以肆无忌惮做这种work的时间段只有achievement差不多时候的PHD year4/5了？</p>
<p>扯的有点多，说回GCN吧。总的看来，其实各种technique/theory还是在发展中的，不过里面确实还有很多可以发掘的东西，而且从功利化发paper的角度，不仅仅是Common GCN, 很多子分支，譬如point cloud，譬如heteogeneous graph（参考<a href="https://wadmes.github.io/2019/12/17/heterogenous network/">https://wadmes.github.io/2019/12/17/heterogenous%20network/</a>） 这些子分支都是水paper的好方向哪。。</p>
<p>另外就是虽然所有paper的conclusion或者method看起来很简单：这不是本科生都能想到的吗？但是里面的story writing绝对是我得好好研究的：怎么将一个简单的solution包装的很elegant，它解决什么问题的？有什么理论去支撑的？实验怎么设置？这里面都是学问啦。总之，虽然novelty不够，但是发现了这些work的一些共同点：</p>
<ul>
<li>大部分都有看起来牛逼哄哄的theorem/proof（对不起因为大部分theorem我没有仔细看，所以只能暂时说看起来。。。）</li>
<li>results部分不会特别出彩，但肯定不是完全比不上别人。1% improvement 🐶，当然如果是专为解决特定问题的，譬如logical问题的那位，就是可以完爆了。</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/GNN/" rel="tag"># GNN</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/17/How-to-use-grammarly-to-check-your-paper/" rel="prev" title="How to use grammarly to check your paper">
      <i class="fa fa-chevron-left"></i> How to use grammarly to check your paper
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/20/Hey-Kong%EF%BC%8CJuly%EF%BC%8C2020/" rel="next" title="Hey Kong，July，2020">
      Hey Kong，July，2020 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#开山老祖-GCN"><span class="nav-number">1.</span> <span class="nav-text">开山老祖 GCN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#方法层面"><span class="nav-number">2.</span> <span class="nav-text">方法层面</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GraphSAGE"><span class="nav-number">2.1.</span> <span class="nav-text">GraphSAGE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GAT"><span class="nav-number">2.2.</span> <span class="nav-text">GAT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JKNet"><span class="nav-number">2.3.</span> <span class="nav-text">JKNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DeepGCN"><span class="nav-number">2.4.</span> <span class="nav-text">DeepGCN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropedge"><span class="nav-number">2.5.</span> <span class="nav-text">Dropedge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Position-aware-Graph-Neural-Networks"><span class="nav-number">2.6.</span> <span class="nav-text">Position-aware Graph Neural Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#COLORING-GRAPH-NEURAL-NETWORKS-FOR-NODE-DISAMBIGUATION"><span class="nav-number">2.7.</span> <span class="nav-text">COLORING GRAPH NEURAL NETWORKS FOR NODE DISAMBIGUATION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GeomGCN"><span class="nav-number">2.8.</span> <span class="nav-text">GeomGCN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generalization-and-Representational-Limits-of-Graph-Neural-Networks"><span class="nav-number">2.9.</span> <span class="nav-text">Generalization and Representational Limits of Graph Neural Networks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#理论层面"><span class="nav-number">3.</span> <span class="nav-text">理论层面</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GIN"><span class="nav-number">3.1.</span> <span class="nav-text">GIN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#THE-LOGICAL-EXPRESSIVENESS-OF-GRAPH-NEURAL-NETWORKS"><span class="nav-number">3.2.</span> <span class="nav-text">THE LOGICAL EXPRESSIVENESS OF GRAPH NEURAL NETWORKS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#WHAT-GRAPH-NEURAL-NETWORKS-CANNOT-LEARN-DEPTH-VS-WIDTH"><span class="nav-number">3.3.</span> <span class="nav-text">WHAT GRAPH NEURAL NETWORKS CANNOT LEARN: DEPTH VS WIDTH</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Approximation-Ratios-of-Graph-Neural-Networks-for-Combinatorial-Problems"><span class="nav-number">3.4.</span> <span class="nav-text">Approximation Ratios of Graph Neural Networks for Combinatorial Problems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Distance-Encoding-–-Design-Provably-More-Powerful-Graph-Neural-Networks-for-Structural-Representation-Learning"><span class="nav-number">3.5.</span> <span class="nav-text">Distance Encoding – Design Provably More Powerful Graph Neural Networks for Structural Representation Learning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#后记"><span class="nav-number">4.</span> <span class="nav-text">后记</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Li Wei"
      src="/images/liwei3.jpg">
  <p class="site-author-name" itemprop="name">Li Wei</p>
  <div class="site-description" itemprop="description">人生注定是一个人的旅行，但就算孤独也要做一直前行的人啊</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">100</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wadmes" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wadmes" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:werry715@gmail.com" title="E-Mail → mailto:werry715@gmail.com" rel="noopener" target="_blank"><i class="fas fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://music.163.com/#/playlist?id=49000450" title="Music → https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;playlist?id&#x3D;49000450" rel="noopener" target="_blank"><i class="fas fa-music fa-fw"></i>Music</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://steamcommunity.com/id/wadmes/" title="Steam → https:&#x2F;&#x2F;steamcommunity.com&#x2F;id&#x2F;wadmes&#x2F;" rel="noopener" target="_blank"><i class="fab fa-steam fa-fw"></i>Steam</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="link fa-fw"></i>
      Interesting links!
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://wadmes.github.io/2019/12/11/EDA-family-tree/" title="https:&#x2F;&#x2F;wadmes.github.io&#x2F;2019&#x2F;12&#x2F;11&#x2F;EDA-family-tree&#x2F;">EDA family tree</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://edu.sina.com.cn/gaokao/2015-05-13/1618468681.shtml" title="http:&#x2F;&#x2F;edu.sina.com.cn&#x2F;gaokao&#x2F;2015-05-13&#x2F;1618468681.shtml" rel="noopener" target="_blank">小爷当年的高考满分作文</a>
        </li>
    </ul>
  </div>

      </div>
      <br />
      <br />
      <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=mzozg_QTtPPn54DeiLT36fT7NX8MObFLeRSVTDmT43Y"></script>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Li Wei</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='50' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'NmNqnbed4p8koRJWwGm6SdrW-gzGzoHsz',
      appKey     : 'ECnYjjxnwouXPYlJjrjLJhv9',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
